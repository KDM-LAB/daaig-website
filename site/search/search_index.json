{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DAAIG","text":""},{"location":"#introduction","title":"Introduction","text":"<p>DAAIG \u2014 Dhirubhai Ambani Artificial Intelligence Group </p> <p>The primary agenda of this group is to foster research collaboration and knowledge sharing in the field of Artificial Intelligence (AI) and Machine Learning (ML) through the following activities:</p> <ul> <li>Workshops on AI/ML \u2013 Inviting eminent researchers to share their insights.</li> <li>Paper Discussions \u2013 Presentations and discussions on recent research papers.</li> <li>Research Debates \u2013 Engaging in critical discussions on AI/ML advancements.</li> <li>Open Discussions \u2013 Addressing challenges and brainstorming solutions collaboratively.</li> <li>PhD Colloquium \u2013 In-depth technical talks by researchers.</li> </ul>"},{"location":"#why-join-daaig","title":"Why Join DAAIG?","text":"<ul> <li>Cross-Domain Insights: Gain unique perspectives by exploring research across different fields.</li> <li>Collaborative Environment: Connect with fellow researchers to brainstorm and build impactful projects.</li> <li>Research-First Culture: A space designed to fuel academic exploration and practical AI development.</li> <li>Stay Informed: Get curated updates on trending AI breakthroughs and emerging technologies.</li> </ul>"},{"location":"team/","title":"Meet the DAAIG Team","text":"<p>Meet the dedicated DAAIG team at DA-IICT Gandhinagar.</p> <p>Dhiraj Golhar</p> <p>Ph.D. Scholar, Convenor </p> <p>DA-IICT Gandhinagar</p> <p>Bhargav Dave</p> <p>Ph.D. Scholar, Deputy Convenor</p> <p>DA-IICT Gandhinagar</p> <p>Bhavin Makwana</p> <p>Ph.D. Scholar</p> <p>DA-IICT Gandhinagar</p> <p>Shivam Gandha</p> <p>Ph.D. Scholar</p> <p>DA-IICT Gandhinagar</p> <p>Shivam Sonawane</p> <p>Ph.D. Scholar</p> <p>DA-IICT Gandhinagar</p> <p>Parthiv Chatterjee</p> <p>Ph.D. Scholar</p> <p>DA-IICT Gandhinagar</p>"},{"location":"llm-workshop-2025/","title":"About","text":""},{"location":"llm-workshop-2025/#1st-daaig-workshop-on-modern-llms-and-applications","title":"1\ud835\udc60\ud835\udc61 DAAIG Workshop on Modern LLMs and Applications.","text":"Dates: 22-23 March, 2025 Time: 9:30 AM - 6:00 PM Where: DA-IICT, DA-IICT Road, Gandhinagar 382 007, Gujarat (India)"},{"location":"llm-workshop-2025/#about","title":"About","text":"<ul> <li>Hands-on Focus \u2013 A two-day immersive workshop designed for practical experience with Large Language Models (LLMs).</li> <li>Less Theory, More Practice \u2013 Instead of long research talks, the emphasis is on interactive coding sessions and real-world applications.</li> <li>Key topics covered:<ul> <li>Fine-tuning LLMs for specific tasks</li> <li>Deploying models efficiently</li> <li>Optimizing performance and reducing costs</li> <li>Mastering prompt engineering and model customization  </li> </ul> </li> <li>For All Skill Levels \u2013 Whether you\u2019re a beginner or   an advanced user, you\u2019ll gain valuable insights into   working with LLMs effectively.</li> <li>Learn by Doing \u2013 Engage in expert-led sessions,   collaborative exercises, and hands-on coding to build   your own AI-powered solutions.   Join us and take your LLM skills to the next level!</li> </ul>"},{"location":"llm-workshop-2025/#audience","title":"Audience","text":"<ol> <li> <p>Data Scientists and Machine Learning Engineers: Professionals seeking to enhance their technical skills in fine-tuning, deploying, and optimizing LLMs efficiently for various applications.</p> </li> <li> <p>Software Developers: Developers interested in mastering LLM applications and model customization to integrate AI solutions into their projects.</p> </li> <li> <p>Beginners and Novices: Those new to the field looking for a hands-on introduction to LLMs that balances learning with practical application, regardless of prior experience level.</p> </li> <li> <p>Academics and Researchers: Individuals in academic settings who prefer interactive, practice-oriented learning over traditional theoretical research talks.</p> </li> </ol>"},{"location":"llm-workshop-2025/#venue","title":"Venue","text":"<p>DAIICT Gandhinagar Gujarat</p>"},{"location":"llm-workshop-2025/#lodging","title":"Lodging","text":"<p>There are a multitude of great lodging options across Gandhinagar. </p> <p></p>"},{"location":"llm-workshop-2025/#code-of-conduct","title":"Code of Conduct","text":"<p>DAIICT and DAAIG are committed to providing a safe, productive, and welcoming environment for all participants in any conference, workshop, field project or project hosted or managed by DAIICT, no matter what role they play or their background.</p>"},{"location":"llm-workshop-2025/registration/","title":"Registration","text":"<p>Thank you for your interest in attending the LLM Workshop!</p> <p>Registration Fee:</p> Registration(per-head) Offline Fees with 35% discount Online Fees DAIICT students INR 500 INR 500 Student INR 1625 INR 1000 Faculty INR 3250 INR 2000 Industry INR 6500 INR 4000 <p>The registration fee for main conference will cover registration kit, full access to all sessions/invited talks, lunch and tea. Note: Registration does not cover accommodation and the participants have to make their own arrangements.</p> <p>Payment Instructions: 1. Pay the registration fee before filling out the registration form. A unique Payment ID/Transaction ID will be generated upon payment, which must be mentioned in the registration form. 2. Confirmation: Upon verification of your Payment ID/Transaction ID, you'll receive a confirmation email and payment receipt within five working days.  </p> <p>Contact us at dactl_events@daiict.ac.in if you do not receive confirmation within the stipulated period. Note: Fees once paid is non-refundable.  </p> <p>Step 1: Pay using following url</p> <p>Step 2: After making the payment, please fill out the registration form</p>"},{"location":"llm-workshop-2025/schedule/","title":"Conference Schedule","text":""},{"location":"llm-workshop-2025/schedule/#day-1","title":"Day 1","text":"Time Speaker Title Abstract 9:00 AM - 9:30 AM REGISTRATION 9:30 AM - 11:00 AM Prof. Preslav Nakov Towards Truly Open, Language-Specific, Safe, Factual, and Specialized Large Language Models View Abstract First, we will argue for the need for fully transparent open-source large language models (LLMs), and we will describe the efforts of MBZUAI's Institute on Foundation Models (IFM) towards that based on the LLM360 initiative. Second, we will argue for the need for language-specific LLMs, and we will share our experience from building Jais, the world's leading open Arabic-centric foundation and instruction-tuned large language model, Nanda, our open-weights Hindi LLM, Sherkala, our open-weights Kazakh LLM, and some other models. Third, we will argue for the need for safe LLMs, and we will present Do-Not-Answer, a dataset for evaluating the guardrails of LLMs, which is at the core of the safety mechanisms of our LLMs. Fourth, we will argue for the need for factual LLMs, we will discuss the factuality challenges that LLMs pose. We will then present some recent relevant tools for addressing these challenges developed at MBZUAI: (i) OpenFactCheck, a framework for fact-checking LLM output, for building customized fact-checking systems, and for benchmarking LLMs for factuality, (ii) LM-Polygraph, a tool for predicting an LLM's uncertainty in its output using cheap and fast uncertainty quantification techniques, and (iii) LLM-DetectAIve, a tool for machine-generated text detection. Finally, we will argue for the need for specialized models, and we will present the zoo of LLMs currently being developed at MBZUAI's IFM.  11:00 PM - 11:15 PM TEA BREAK 11:15 AM - 12:45 PM Prof. Tanmoy Chakraborty Don't underestimate the power of small language models View Abstract Despite the superior performance demonstrated by Transformer-based LLMs across numerous applications involving natural languages, their high computational cost, energy consumption, and limited accessibility underscore the need for efficient, interpretable, and adaptable small language models (SLMs). This talk highlights methods to develop economical and interpretable SLMs that rival their larger counterparts in performance without significant computational requirements. Our research emphasizes three key dimensions: economical resource usage, adaptability to diverse and low-resource tasks, and enhanced interpretability. Techniques like competitive knowledge distillation, leveraging student-teacher dynamics, and activation sparsity in manifold-preserving transformers demonstrate significant efficiency gains without compromising performance. We formulate novel decomposer components for LLMs for modularizing problem decomposition and solution generation, allowing smaller models to excel in complex reasoning tasks. We also propose innovative prompt construction and alignment strategies that boost in-context knowledge adaptation in low-resource settings for SLMs. Our findings demonstrate that SLMs can achieve scalability, interpretability, and adaptability, paving the way for broader and sustainable AI accessibility.  12:45 PM - 1:45 PM LUNCH 1:45 PM - 3:45 PM Sahil Mishra Retrieval-Augmented Language Models \u2013 Bridging LLMs with Efficient Knowledge Retrieval View Abstract Large Language Models (LLMs) are powerful but have limitations like forgetting recent information and hallucination. Retrieval-Augmented Language Models (RAG) solve these problems by allowing models to fetch relevant information from external sources instead of relying only on what they were trained on. This session will cover how retrieval-based models work, the different ways they retrieve information (like using sparse and dense retrieval methods), and how they improve accuracy and efficiency. We will explore models like kNN-LMs, REALM, RETRO, and RAG, showing how they use retrieval to enhance responses. Additionally, we will discuss strategies for improving retrieval, aligning retrieved knowledge with model outputs, and refining prompts for better results, especially in low-resource settings. By combining retrieval with language models, we can build smaller, more efficient, and more reliable AI systems that provide accurate, well-supported answers in real-world applications.  3:45 PM - 4:00 PM TEA BREAK 4:00 PM - 6:00 PM Ankush Chander LLM finetuning: Fundamentals and best practices View Abstract Large language models have transformed the field of NLP by performing well on tasks that were previously not reachable. Even though LLMs have great general language capabilities, sometimes it's not enough for the application specific tasks. Fine-tuning allows users to adapt pre-trained LLMs to more specialized tasks. By fine-tuning a model on a small dataset of task-specific data, you can improve its performance on that task while preserving its general language knowledge. In this session, we will discuss finetuning basics, memory optimization techniques like Quantization, LoRA and finetune some LLMs along the way."},{"location":"llm-workshop-2025/schedule/#day-2","title":"Day 2","text":"Time Speakers Titles Abstract 9:00 AM - 10:30 AM Harsha Kokel A brief tutorial on LLMs for AI Planning View Abstract Recent work advancements in Large Language Models (LLMs), have spurred approaches for planning in natural language. The approaches vary widely from giving a planning problem to an LLM and asking it to output an entire plan to asking an LLM to plan step by step, including backtracking. In this talk, I will give a brief overview of AI Planning and how the advancements in LLMs are being used for planning in natural language. I will cover a few different approaches, discuss important properties to consider, and present some benchmarks for evaluation.  10:30 PM - 10:45 PM TEA BREAK 10:45 AM - 12:15 PM Sandipan Dandapat Safe and Inclusive AI for a Multilingual and Multicultural World View Abstract In this talk, I will explore the challenges and innovations in large language models. I will delve into the complexities of scaling language models, addressing issues such as power versus cost and the responsibilities associated with powerful AI. The talk will then describe two research works in detail. The first focuses on SAGE: Safety AI Generic Evaluation, a novel approach to ensuring AI safety and examining the biases and stereotypes in large language models. The second research work will present the Linguistically Informed Testing of Multilingual Systems (LITMUS) project, which aims to support universalization through linguistically informed training and testing strategies. Finally, I will conclude with an outlook and future directions for work in this field.  12:15 PM - 1:30 PM LUNCH 1:30 PM - 3:00 PM Manish Gupta A Brief tutorial on Retrieval Augmented Generation View Abstract In this talk, I will introduce the recently popular concept of retrieval augmented generation. We will start with retrieval augmentation for classification (REALM) and then extend the framework to generation (RAG). Then we will deliberate on how to scale these models to trillion-sized collections (RETRO) and how to combine retrieval-augmentation with few-shot learning (ATLAS). Lastly, I will talk about Internet-augmented generation, and application of RAG in AutoSuggest. Towards the end, I will also briefly touch upon multimodal RAG.  3:00 PM - 3:15 PM TEA BREAK 3:15 PM - 5:15 PM Harish Yenala Practical Fine-Tuning of SLMs: Techniques, Applications, and Performance Insights View Abstract This hands-on session will focus on fine-tuning Small Language Models (SLMs) for specific applications. We'll cover the importance of SLMs, their limitations, and the need for fine-tuning using methods like Parameter-Efficient Fine-Tuning (PEFT) and QLoRA. Through a real-world example on Hate Speech Text Classification, participants will gain insights into data preparation for fine-tuning, the process of fine-tuning the Phi-3 model, and evaluating its impact. We will also touch upon the performance and latency comparisons between various SLM models, showcasing their practical implications in real-time applications."},{"location":"llm-workshop-2025/speakers/","title":"Speakers","text":""},{"location":"llm-workshop-2025/speakers/#invited-speakers","title":"Invited Speakers","text":"<p>Prof. Preslav Nakov</p> <p>MBZUAI</p> <p>Tanmoy Chakraborty</p> <p>IIT Delhi</p> <p>Sandipan Dandapat</p> <p>Microsoft Research</p> <p>Manish Gupta</p> <p>Microsoft Research</p> <p>Harsha Kokel</p> <p>IBM Research</p>"},{"location":"llm-workshop-2025/speakers/#hands-on-speakers","title":"Hands-on Speakers","text":"<p>Harish Yenala</p> <p>Microsoft</p> <p>Sahil Mishra</p> <p>IIT Delhi</p> <p>Ankush Chander</p> <p>DAU</p>"}]}